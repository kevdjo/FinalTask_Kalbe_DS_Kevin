# -*- coding: utf-8 -*-
"""PBI Rakamin X Kalbe - Kevin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s_sJcy5K1gZvLhIoSl9w0S6UOKaUOeOG
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from itertools import product

from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from statsmodels.tsa.arima.model import ARIMA
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import pmdarima as pm
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score
from yellowbrick.cluster import KElbowVisualizer

import plotly as py
import plotly.graph_objs as go

import warnings
warnings.filterwarnings('ignore')

!pip install pmdarima

"""# Load Data

"""

data_Customer = pd.read_csv('Customer.csv', sep=';')
data_Product = pd.read_csv('Product.csv', sep=';')
data_Store = pd.read_csv('Store.csv', sep=';')
data_Transaction = pd.read_csv('Transaction.csv', sep=';')

data_Customer.head()

data_Customer.info()

data_Product.head()

data_Product.info()

data_Store.head()

data_Store.info()

data_Transaction.head()

data_Transaction.info()

"""# Data Merged and Cleansing

"""

data_temp1 = pd.merge(data_Transaction, data_Customer, on='CustomerID', how='inner')
data_temp2 = pd.merge(data_temp1, data_Store, on = 'StoreID', how = 'inner')
df_merged = pd.merge(data_temp2, data_Product, on = 'ProductID', how = 'inner')
df_merged.head()

df_merged.info()

(df_merged['Price_x']/df_merged['Price_y']).value_counts()

df_merged.drop(columns ='Price_y', inplace = True)

df_merged.isna().sum()

df_merged.dropna(inplace = True)
df_merged.isna().sum()

df_merged.duplicated().sum()

df_merged.info()

df_merged['TransactionID'] = df_merged['TransactionID'].astype('string')
df_merged['CustomerID'] = df_merged['CustomerID'].astype('string')
df_merged['Date'] = pd.to_datetime(df_merged['Date'], format='%d/%m/%Y')
df_merged['ProductID'] = df_merged['ProductID'].astype('string')
df_merged['StoreName'] = df_merged['StoreName'].astype('string')
df_merged['GroupStore'] = df_merged['GroupStore'].astype('string')
df_merged['Type'] = df_merged['Type'].astype('|S')
df_merged['Longitude'] = df_merged['Longitude'].apply(lambda x: x.replace(',','.')).astype(float)
df_merged['Latitude'] = df_merged['Latitude'].apply(lambda x: x.replace(',','.')).astype(float)
df_merged['Marital Status'] = df_merged['Marital Status'].astype('string')
df_merged['Income'] = df_merged['Income'].apply(lambda x: x.replace(',','.')).astype(float)
df_merged['Product Name'] = df_merged['Product Name'].astype('string')

df_merged.info()

df_merged.head()

"""# **Time Series Forecasting**

**Time Series Analysis**
"""

df_arima = df_merged.groupby('Date', as_index =True)['Qty'].sum()
df_arima = pd.DataFrame(df_arima)
df_arima.head()

df_arima.plot(figsize=(18,3), title='Time Series Visualisation')

print(f"Orignal Time Series Data Shape = {df_arima.shape}")
test_size = round(df_arima.shape[0] * 0.2)
train=df_arima.iloc[:-1*(test_size)]
test=df_arima.iloc[-1*(test_size):]
print(f"Training Time Series Data Shape = {train.shape}")
print(f"Test Time Series Data Shape = {test.shape}")

plt.figure(figsize=(10,3))
sns.lineplot(data=train, x=train.index, y=train['Qty'])
sns.lineplot(data=test, x=test.index, y=test['Qty'], color='green')
plt.title('Train vs Test Time Series Data')
plt.show()

"""
**Check for Stationary**"""

def adf_test(timeseries):
    #Perform Dickey-Fuller test:
    print ('Results of Dickey-Fuller Test:')
    dftest = adfuller(timeseries, autolag = 'AIC')
    dfoutput = pd.Series(dftest[0:4], index = ['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
    for key,value in dftest[4].items():
       dfoutput['Critical Value (%s)'%key] = value
    print (dfoutput)
    if dftest[1] < 0.05:
      print("Strong objection towards Null Hypothesis. This indicate time series has no unit root, therefore it is stationary")
    else:
      print("Weak to reject Null Hypothesis. This indicate time series has unit root, therefore it is non stationary")

adf_test(df_arima)

fig, ax = plt.subplots(1, 2, figsize=(12, 4))
plot_acf(df_arima.diff().dropna(), lags=40, ax=ax[0])
plot_pacf(df_arima.diff().dropna(), lags=40, ax=ax[1])
plt.show()

"""**Modelling**

**Auto Arima**
"""

model_auto = pm.auto_arima(train,test='adf',
                      seasonal=False,
                      trace=True,
                      error_action='ignore',
                      suppress_warnings=True,
                      stepwise=False)

print(model_auto.summary())

"""**Hyperparameter Tuning**"""

p = range(0, 5)
d = range(0, 5)
q = range(0, 5)

pdq = list(product(p, d, q))
print(pdq)

from statsmodels.tsa.arima.model import ARIMA
aic_scores = []
for param in pdq:
    model = ARIMA(df_arima, order=param)
    model_fit = model.fit()
    aic_scores.append({'par': param, 'aic': model_fit.aic})

best_aic = min(aic_scores, key=lambda x: x['aic'])
print(best_aic)

model_hyper = ARIMA(train, order=best_aic['par'])
model_fit_hyper = model_hyper.fit()

"""**Manual Hyperparameter**"""

model_manual = ARIMA(train, order=(40,2,2))
model_fit_manual = model_manual.fit()

"""**Plot Forecasting**"""

manual_forecast = model_fit_manual.forecast(len(test))
ht_forecast = model_fit_hyper.forecast(len(test))
auto_forecast = model_auto.predict(len(test))

df_plot = df_arima.iloc[-100:]

df_plot['forecast_manual'] = [None]*(len(df_plot)-len(manual_forecast)) + list(manual_forecast)
df_plot['forecast_hyper'] = [None]*(len(df_plot)-len(ht_forecast)) + list(ht_forecast)
df_plot['forecast_auto'] = [None]*(len(df_plot)-len(auto_forecast)) + list(auto_forecast)

df_plot.plot(figsize=(15,4))
plt.show()

"""**Metrics Evaluation**"""

mae = mean_absolute_error(test, manual_forecast)
mape = mean_absolute_percentage_error(test, manual_forecast)
rmse = np.sqrt(mean_squared_error(test, manual_forecast))

print(f'Mean Absolute Error - manual            : {round(mae,4)}')
print(f'Mean Absolute Percentage Error - manual : {round(mape,4)}')
print(f'Root Mean Square Error - manual         : {round(rmse,4)}')

"""**Forecasting Quantity Sales with Best Parameter**"""

model = ARIMA(df_arima, order=(40, 2, 2))
model_fit = model.fit()
arima_forecast = model_fit.forecast(steps=31)

arima_forecast

plt.figure(figsize=(15,3))
plt.plot(df_arima)
plt.plot(arima_forecast,color='red')
plt.title('Quantity Sales Forecasting')
plt.show()

"""# **Clustering**"""

df_merged.head()

df_clust = df_merged.groupby('CustomerID').agg({'TransactionID':'count',
                                                   'Qty':'sum',
                                                   'TotalAmount':'sum'}).reset_index()
df_clust

df_clust.info()

df_cluster = df_clust.drop(columns = ['CustomerID'])
df_cluster.head()

df_cluster.info()

df_cluster.isna().sum()

X = df_cluster.values
X_std = StandardScaler().fit_transform(X)
df_std = pd.DataFrame(data=X_std,columns=df_cluster.columns)
df_std.isna().sum()

X_std

df_std

wcss= []
for n in range (1,11):
    model1 = KMeans(n_clusters=n, init='k-means++', n_init = 10, max_iter=100, tol =0.0001, random_state = 100)
    model1.fit(X_std)
    wcss.append(model1.inertia_)
print(wcss)

plt.figure(figsize=(7,4))
plt.plot(list(range(1,11)), wcss, color = 'blue', marker = 'o', linewidth=2, markersize=7, markerfacecolor= 'm',
         markeredgecolor= 'm')
plt.title('WCSS vs Number of Cluster')
plt.xlabel('Number of Cluster')
plt.ylabel('WCSS')
plt.xticks(list(range(1,11)))
plt.show()

#Elbow Method with yellowbrick library
visualizer = KElbowVisualizer(model1, k=(2,10))
visualizer.fit(X_std)
visualizer.show()

K = range(2,8)
fits=[]
score=[]

for k in K:
    model = KMeans(n_clusters = k, random_state = 0, n_init= 'auto').fit(X_std)
    fits.append(model)
    score.append(silhouette_score(X_std, model.labels_, metric='euclidean'))

sns.lineplot(x = K, y = score)

# Kmeans n_cluster = 4
#Clustering Kmeans
kmeans_4 = KMeans(n_clusters=4,init='k-means++',max_iter=300,n_init=10,random_state=100)
kmeans_4.fit(X_std)

# Masukin cluster ke dataset
df_cluster['cluster'] = kmeans_4.labels_
df_cluster.head()

plt.figure(figsize=(6,6))
sns.pairplot(data=df_cluster,hue='cluster',palette='Set1')
plt.show()

df_cluster['CustomerID'] = df_clust['CustomerID']
df_cluster_mean = df_cluster.groupby('cluster').agg({'CustomerID':'count','TransactionID':'mean','Qty':'mean','TotalAmount':'mean'})
df_cluster_mean.sort_values('CustomerID', ascending = False)

"""**Cluster 0:**

Cluster dengan jumlah pelanggan terbanyak diantara cluster lainnnya. Cluster ini menempati posisi 3 tertinggi dari penilaian tiap fitur yang ada. Strategi yang dapat diterapkan untuk cluster pelanggan ini ialah sebagai berikut:
*   Memberikan bonus poin sebagai keterlibatan mereka sebagai member loyal untuk mempertahankan transaksi dan juga meningkatkan peminatan.
*   Melakukan survei secara berkala supaya dapat mengetahui kepuasan dan penat cluster pelanggan ini

**Cluster 1:**

Cluster dengan jumlah pelanggan ketiga paling banyak diantara cluster lainnnya. Cluster ini menempati posisi terendah dari penilaian tiap fitur yang ada. Strategi yang dapat diterapkan untuk cluster pelanggan ini ialah sebagai berikut:
*   Menawarkan paket promo yang menarik untuk meningkatkan peminatan cluster pelanggan untuk bertransaksi
*   Melakukan survei secara intens untuk dapat mempetakan sentimen cluster pelanggan terhadap produk
*   Menyediakan layanan customer service untuk membantu cluster pelanggan memahami produk-produk lebih baik

**Cluster 2:**

Cluster dengan jumlah pelanggan terendah diantara cluster lainnnya. Cluster ini menempati posisi tertinggi dari penilaian tiap fitur yang ada. Strategi yang dapat diterapkan untuk cluster pelanggan ini ialah sebagai berikut:
*   Menawarkan program membership untuk mempertahankan atau bahkan meningkatkan transaksi.
*   Memberikan update atau info terkini lebih cepat dari pada cluster lainnya supaya dapat meningkatkan antusias terhadap produk yang ada.

**Cluster 3:**
Cluster dengan jumlah pelanggan kedua paling banyak diantara cluster lainnnya. Cluster ini menempati posisi kedua paling tinggi dari penilaian tiap fitur yang ada. Strategi yang dapat diterapkan untuk cluster pelanggan ini ialah sebagai berikut:
*   Menerapkan produk upselling
*   Mengadakan promo-promo menarik untuk mempertahankan transaksi
*   Memasarkan program membership untuk meningkatkan minat bertransaksi

"""

trace1 = go.Scatter3d(
    x= df_cluster['TransactionID'],
    y= df_cluster['Qty'],
    z= df_cluster['TotalAmount'],
    mode='markers',
     marker=dict(
        color = df_cluster['cluster'],
        size= 10,
        line=dict(
            color= df_cluster['cluster'],
            width= 12
        ),
        opacity=0.8
     )
)
data = [trace1]
layout = go.Layout(
    title= 'Clusters wrt TransactionID, Qty and Spending TotalAmount',
    scene = dict(
            xaxis = dict(title  = 'TransactionID Count'),
            yaxis = dict(title  = 'Quantity'),
            zaxis = dict(title  = 'Total Amount')
        )
)
fig = go.Figure(data=data, layout=layout)
py.offline.iplot(fig)